{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdzrLAtZTBwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77130e5-fd6a-4d57-a5c4-13c93f1285c7"
      },
      "source": [
        "import urllib\n",
        "import re\n",
        "import numpy as np\n",
        "from random import uniform\n",
        "from urllib.request import urlopen\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Color:\n",
        "    PURPLE = '\\033[95m'\n",
        "    CYAN = '\\033[96m'\n",
        "    DARKCYAN = '\\033[36m'\n",
        "    BLUE = '\\033[94m'\n",
        "    GREEN = '\\033[92m'\n",
        "    YELLOW = '\\033[93m'\n",
        "    RED = '\\033[91m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "    END = '\\033[0m'\n",
        "\n",
        "\n",
        "def read_text_file(url):\n",
        "    # Open text-file and return a separated input and output variable(series of words)\n",
        "    text_file = urllib.request.urlopen(url)\n",
        "    file_data = text_file.read()\n",
        "    file_data = file_data.decode('utf8').lower()\n",
        "    file_chars = set(file_data)\n",
        "    file_data_size, file_vocab_size = len(file_data), len(file_chars)\n",
        "    return file_data, file_data_size, file_vocab_size, file_chars\n",
        "\n",
        "\n",
        "# Sigmoid/ Sigmoid Derivative/Tangent/ Tangent Derivative/Softmax function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def d_sigmoid(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "\n",
        "def tangent(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def d_tangent(self, x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    return (np.exp(x - np.max(x))) / np.sum(np.exp(x - np.max(x)))\n",
        "\n",
        "\n",
        "class LSTM:\n",
        "    # Class constructor\n",
        "    def __init__(self, char_map, idx_map, unique_num, iterations=10, learning_r=0.01, firstBeta=0.9, secondBeta=0.999):\n",
        "        # Number of hidden layers and Batch Size\n",
        "        hiddenNum = 100\n",
        "        batchSize = 25\n",
        "\n",
        "        # Initiate parameters, gradient, and parameters for adam dictionaries\n",
        "        self.parameters = {}\n",
        "        self.gradients = {}\n",
        "        self.adam = {}\n",
        "\n",
        "        # First and Second Beta, initiated at 0.9 and 0.999\n",
        "        self.firstBeta = firstBeta\n",
        "        self.secondBeta = secondBeta\n",
        "\n",
        "        # Batch size\n",
        "        self.batch_size = batchSize\n",
        "\n",
        "        # Number of Epochs, initiated at 10\n",
        "        self.epochNum = iterations\n",
        "\n",
        "        # Learning rate, initiated at 0.01\n",
        "        self.learningRate = learning_r\n",
        "\n",
        "        # charMap and indexMap: Character and Index Mapping\n",
        "        self.charMap = char_map\n",
        "        self.indexMap = idx_map\n",
        "\n",
        "        # Number of Hidden Layers and unique characters\n",
        "        self.uniqueNum = unique_num\n",
        "        self.hiddenNum = hiddenNum\n",
        "\n",
        "        # Initialize Xavier function\n",
        "        xavier = (1.0 / np.sqrt(self.uniqueNum + self.hiddenNum))\n",
        "\n",
        "        # Weight and Beta key Initiation. This includes forget, cell, Output, and total parameters\n",
        "        self.parameters[\"WeightForget\"] = np.random.randn(self.hiddenNum, self.hiddenNum + self.uniqueNum) * xavier\n",
        "        self.parameters[\"betaForget\"] = np.ones((self.hiddenNum, 1))\n",
        "\n",
        "        self.parameters[\"WeightInput\"] = np.random.randn(self.hiddenNum, self.hiddenNum + self.uniqueNum) * xavier\n",
        "        self.parameters[\"betaInput\"] = np.zeros((self.hiddenNum, 1))\n",
        "\n",
        "        self.parameters[\"WeightCell\"] = np.random.randn(self.hiddenNum, self.hiddenNum + self.uniqueNum) * xavier\n",
        "        self.parameters[\"betaCell\"] = np.zeros((self.hiddenNum, 1))\n",
        "\n",
        "        self.parameters[\"WeightOutput\"] = np.random.randn(self.hiddenNum, self.hiddenNum + self.uniqueNum) * xavier\n",
        "        self.parameters[\"betaOutput\"] = np.zeros((self.hiddenNum, 1))\n",
        "\n",
        "        self.parameters[\"WeightTotal\"] = np.random.randn(self.uniqueNum, self.hiddenNum) * \\\n",
        "                                         (1.0 / np.sqrt(self.uniqueNum))\n",
        "        self.parameters[\"betaTotal\"] = np.zeros((self.uniqueNum, 1))\n",
        "\n",
        "        # Initiate derivative, meter, and value for Weight and Beta key.\n",
        "        for gate in self.parameters:\n",
        "            self.gradients[\"derivative\" + gate] = np.zeros_like(self.parameters[gate])\n",
        "            self.adam[\"meter\" + gate] = np.zeros_like(self.parameters[gate])\n",
        "            self.adam[\"value\" + gate] = np.zeros_like(self.parameters[gate])\n",
        "\n",
        "        self.lossSmooth = -np.log(1.0 / self.uniqueNum) * self.batch_size\n",
        "        return\n",
        "\n",
        "    # Clip function to limit gradients to conservative value\n",
        "    def gradientClip(self):\n",
        "        for gate in self.gradients:\n",
        "            np.clip(self.gradients[gate], -5, 5, out=self.gradients[gate])\n",
        "        return\n",
        "\n",
        "    # Reset the gradient to zero\n",
        "    def gradientClear(self):\n",
        "        for gate in self.gradients:\n",
        "            self.gradients[gate].fill(0)\n",
        "        return\n",
        "\n",
        "    # Using Adam function, update the parameters\n",
        "    def parametersUpdate(self, batch_num):\n",
        "        for gate in self.parameters:\n",
        "            self.adam[\"meter\" + gate] = self.adam[\"meter\" + gate] * self.firstBeta + \\\n",
        "                                        (1 - self.firstBeta) * self.gradients[\"derivative\" + gate]\n",
        "            self.adam[\"value\" + gate] = self.adam[\"value\" + gate] * self.secondBeta + \\\n",
        "                                        (1 - self.secondBeta) * self.gradients[\"derivative\" + gate] ** 2\n",
        "            \n",
        "            # accumulated gradients are updated for all time steps\n",
        "            crl_Meter = self.adam[\"meter\" + gate] / (1 - self.firstBeta ** batch_num)\n",
        "            crl_Value = self.adam[\"value\" + gate] / (1 - self.secondBeta ** batch_num)\n",
        "\n",
        "            self.parameters[gate] -= self.learningRate * crl_Meter / (np.sqrt(crl_Value) + 1e-7)\n",
        "        return\n",
        "\n",
        "    # Generate sample text\n",
        "    def outputResult(self, prev_hidden, prev_cell, sample_size):\n",
        "        # Variable Initiation\n",
        "        inputMatrix = np.zeros((self.uniqueNum, 1))\n",
        "        curr_hidden = prev_hidden\n",
        "        curr_cell = prev_cell\n",
        "        outputLine = \"\"\n",
        "        # Output random lines for the result\n",
        "        for count in range(sample_size):\n",
        "            token, _, curr_hidden, _, curr_cell, _, _, _, _ = self.forwardProp(inputMatrix, curr_hidden, curr_cell)\n",
        "            randomInterval = np.random.choice(range(self.uniqueNum), p=token.ravel())\n",
        "            inputMatrix = np.zeros((self.uniqueNum, 1))\n",
        "            inputMatrix[randomInterval] = 1\n",
        "            tempWord = self.indexMap[randomInterval] # match char with relative index\n",
        "            outputLine += tempWord # concat to final string\n",
        "        return outputLine\n",
        "\n",
        "    # Forward prop function definition\n",
        "    def forwardProp(self, predictor, prev_hidden, prev_cell):\n",
        "        # Weight, forget, input, cell variable initialization\n",
        "        weight = np.row_stack((prev_hidden, predictor))\n",
        "\n",
        "        forget = sigmoid(np.dot(self.parameters[\"WeightForget\"], weight) + self.parameters[\"betaForget\"])\n",
        "\n",
        "        # previous hidden states as input\n",
        "        input_d = sigmoid(np.dot(self.parameters[\"WeightInput\"], weight) + self.parameters[\"betaInput\"])\n",
        "\n",
        "        # previous cell as input\n",
        "        cell = np.tanh(np.dot(self.parameters[\"WeightCell\"], weight) + self.parameters[\"betaCell\"])\n",
        "\n",
        "        cur_cell = forget * prev_cell + input_d * cell\n",
        "        temp_sig = sigmoid(np.dot(self.parameters[\"WeightOutput\"], weight) + self.parameters[\"betaOutput\"])\n",
        "        cur_hidden = temp_sig * np.tanh(cur_cell)\n",
        "\n",
        "        value = np.dot(self.parameters[\"WeightTotal\"], cur_hidden) + self.parameters[\"betaTotal\"]\n",
        "        token = softmax(value)\n",
        "        return token, value, cur_hidden, temp_sig, cur_cell, cell, input_d, forget, weight\n",
        "\n",
        "    # Backward prop function definition\n",
        "    def backwardProp(self, realValue, token, af_d_hidden, af_d_cell, prev_cell, weight, forget, input_d, cell, cur_cell, tempsig, currhidden):\n",
        "        d_total = np.copy(token)\n",
        "        d_total[realValue] -= 1\n",
        "\n",
        "        self.gradients[\"derivativeWeightTotal\"] += np.dot(d_total, currhidden.T)\n",
        "        self.gradients[\"derivativebetaTotal\"] += d_total\n",
        "\n",
        "        d_hidden = np.dot(self.parameters[\"WeightTotal\"].T, d_total)\n",
        "        d_hidden += af_d_hidden\n",
        "\n",
        "        d_Sigmoid = d_hidden * np.tanh(cur_cell)\n",
        "        dsig = d_Sigmoid * tempsig * (1 - tempsig)\n",
        "        self.gradients[\"derivativeWeightOutput\"] += np.dot(dsig, weight.T)\n",
        "        self.gradients[\"derivativebetaOutput\"] += dsig\n",
        "\n",
        "        d_cell = d_hidden * tempsig * (1 - np.tanh(cur_cell) ** 2)\n",
        "        d_cell += af_d_cell\n",
        "\n",
        "        d_cell_backward = d_cell * input_d\n",
        "        d_cell_final = d_cell_backward * (1 - cell ** 2)\n",
        "        self.gradients[\"derivativeWeightCell\"] += np.dot(d_cell_final, weight.T)\n",
        "        self.gradients[\"derivativebetaCell\"] += d_cell_final\n",
        "\n",
        "        d_input = d_cell * cell\n",
        "        d_input_final = d_input * input_d * (1 - input_d)\n",
        "        self.gradients[\"derivativeWeightInput\"] += np.dot(d_input_final, weight.T)\n",
        "        self.gradients[\"derivativebetaInput\"] += d_input_final\n",
        "\n",
        "        d_forget = d_cell * prev_cell\n",
        "        d_forget_final = d_forget * forget * (1 - forget)\n",
        "        self.gradients[\"derivativeWeightForget\"] += np.dot(d_forget_final, weight.T)\n",
        "        self.gradients[\"derivativebetaForget\"] += d_forget_final\n",
        "\n",
        "        # Sum up all gradients\n",
        "        d_final = (np.dot(self.parameters[\"WeightForget\"].T, d_forget_final)\n",
        "              + np.dot(self.parameters[\"WeightInput\"].T, d_input_final)\n",
        "              + np.dot(self.parameters[\"WeightCell\"].T, d_cell_final)\n",
        "              + np.dot(self.parameters[\"WeightOutput\"].T, dsig))\n",
        "\n",
        "        prev_d_hidden = d_final[:self.hiddenNum, :]\n",
        "        prev_d_cell = forget * d_cell\n",
        "        return prev_d_hidden, prev_d_cell\n",
        "\n",
        "    # Forward and Backward Prop Function\n",
        "    def forbackProp(self, batchPred, batchRes, prev_hidden, prev_cell):\n",
        "        # Initiate empty dictionaries to store results\n",
        "        predictor, weight = {}, {}\n",
        "        forget, input__, cell, curr_cell, temp_sig = {}, {}, {}, {}, {}\n",
        "        token, value, curr_hidden = {}, {}, {}\n",
        "        curr_hidden[-1] = prev_hidden\n",
        "        curr_cell[-1] = prev_cell\n",
        "\n",
        "        # Iterate time steps and store results into dictionaries\n",
        "        lossValue = 0\n",
        "        for count in range(self.batch_size):\n",
        "            predictor[count] = np.zeros((self.uniqueNum, 1))\n",
        "            predictor[count][batchPred[count]] = 1\n",
        "\n",
        "            token[count], value[count], curr_hidden[count], temp_sig[count], curr_cell[count], cell[count], input__[\n",
        "                count], forget[count], weight[count] = self.forwardProp(predictor[count], curr_hidden[count - 1],\n",
        "                                                                         curr_cell[count - 1])\n",
        "\n",
        "            lossValue += -np.log(token[count][batchRes[count], 0])\n",
        "\n",
        "        self.gradientClear()\n",
        "\n",
        "        after_d_hidden = np.zeros_like(curr_hidden[0])\n",
        "        after_d_cell = np.zeros_like(curr_cell[0])\n",
        "\n",
        "        # Feeding first LSTM cell to next training batch\n",
        "        for count in reversed(range(self.batch_size)):\n",
        "            after_d_hidden, after_d_cell = self.backwardProp(batchRes[count], token[count], after_d_hidden,\n",
        "                                                              after_d_cell, curr_cell[count - 1], weight[count],\n",
        "                                                              forget[count], input__[count],\n",
        "                                                              cell[count], curr_cell[count], temp_sig[count],\n",
        "                                                              curr_hidden[count])\n",
        "        return lossValue, curr_hidden[self.batch_size - 1], curr_cell[self.batch_size - 1]\n",
        "\n",
        "    # Model train function\n",
        "    def modelTrain(self, data_):\n",
        "        # Initialize loss list, batch count and batchData\n",
        "        loss_list = []\n",
        "        batchCount = len(data_) // self.batch_size\n",
        "        batchData = data_[: batchCount * self.batch_size] # trim chars at the end of input to form complete sequence\n",
        "\n",
        "        # Iterate over training batch to fit length of sequence\n",
        "        for epochCount in range(self.epochNum):\n",
        "            prev_hidden = np.zeros((self.hiddenNum, 1))\n",
        "            prev_cell = np.zeros((self.hiddenNum, 1))\n",
        "\n",
        "            for count in range(0, len(batchData) - self.batch_size, self.batch_size):\n",
        "                batchPred = [self.charMap[ch] for ch in batchData[count: count + self.batch_size]]\n",
        "                batchRes = [self.charMap[ch] for ch in batchData[count + 1: count + self.batch_size + 1]]\n",
        "\n",
        "                lossValue, prev_hidden, prev_cell = self.forbackProp(batchPred, batchRes, prev_hidden, prev_cell)\n",
        "                self.lossSmooth = self.lossSmooth * 0.999 + lossValue * 0.001 # tune up loss result\n",
        "                loss_list.append(self.lossSmooth) # append loss result to loss_list\n",
        "\n",
        "                self.gradientClip() # limit gradient value\n",
        "\n",
        "                batch_num = epochCount * self.epochNum + count / self.batch_size + 1\n",
        "                self.parametersUpdate(batch_num)\n",
        "\n",
        "                # output the result\n",
        "                if count % 100000 == 0:\n",
        "                    print(Color.BOLD, 'Iteration:', epochCount, '\\tBatch:', count, \"-\", count + self.batch_size,\n",
        "                          '\\tLoss:', round(self.lossSmooth, 2), Color.END)\n",
        "                    result = self.outputResult(prev_hidden, prev_cell, sample_size=250)\n",
        "                    print(result, \"\\n\") # print result\n",
        "\n",
        "        return loss_list, self.parameters\n",
        "\n",
        "# Main Program\n",
        "if __name__ == \"__main__\":\n",
        "    path = \"https://datasetdanny.s3.amazonaws.com/alice.txt\"\n",
        "    data, data_size, uniqueNum, chars = read_text_file(path)\n",
        "\n",
        "    print(\"Total Characters: \", data_size)\n",
        "    print(\"Total Vocabulary: \", uniqueNum)\n",
        "\n",
        "    # Creating character/word mappings\n",
        "    charMap = {ch: __input for __input, ch in enumerate(chars)}\n",
        "    indexMap = {__input_: ch for __input_, ch in enumerate(chars)}\n",
        "    print(Color.BOLD, \"Characters/Words mapping\", Color.END)\n",
        "    print(charMap)\n",
        "    print(indexMap)\n",
        "\n",
        "    print(Color.BOLD, \"Initialize parameters.......\", Color.END)\n",
        "    epochNum = 60\n",
        "    learningRate = 0.003\n",
        "\n",
        "    print(Color.BOLD, \"Iterations: \", epochNum, Color.END)\n",
        "    print(Color.BOLD, \"Learning rate: \", learningRate, Color.END)\n",
        "\n",
        "    model = LSTM(charMap, indexMap, uniqueNum, epochNum, learningRate)\n",
        "    lossList, parameters = model.modelTrain(data)\n",
        "\n",
        "    plt.plot([input_data for input_data in range(len(lossList))], lossList)\n",
        "    plt.xlabel(\"Training Iterations\")\n",
        "    plt.ylabel(\"Training lossValue\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  167513\n",
            "Total Vocabulary:  59\n",
            "\u001b[1m Characters/Words mapping \u001b[0m\n",
            "{'t': 0, '4': 1, 'l': 2, 'v': 3, 'q': 4, '#': 5, 'y': 6, ']': 7, '\\r': 8, '5': 9, 'r': 10, 'p': 11, ';': 12, '$': 13, '[': 14, '\\n': 15, 's': 16, 'k': 17, 'n': 18, '%': 19, ',': 20, '3': 21, '(': 22, '/': 23, 'b': 24, \"'\": 25, '2': 26, 'w': 27, ' ': 28, '.': 29, '9': 30, '8': 31, 'i': 32, 'z': 33, 'j': 34, '0': 35, '_': 36, '!': 37, 'd': 38, 'm': 39, 'f': 40, ')': 41, '*': 42, '?': 43, 'u': 44, 'c': 45, '1': 46, '7': 47, '6': 48, 'o': 49, 'g': 50, 'a': 51, '\"': 52, 'h': 53, 'e': 54, '@': 55, ':': 56, '-': 57, 'x': 58}\n",
            "{0: 't', 1: '4', 2: 'l', 3: 'v', 4: 'q', 5: '#', 6: 'y', 7: ']', 8: '\\r', 9: '5', 10: 'r', 11: 'p', 12: ';', 13: '$', 14: '[', 15: '\\n', 16: 's', 17: 'k', 18: 'n', 19: '%', 20: ',', 21: '3', 22: '(', 23: '/', 24: 'b', 25: \"'\", 26: '2', 27: 'w', 28: ' ', 29: '.', 30: '9', 31: '8', 32: 'i', 33: 'z', 34: 'j', 35: '0', 36: '_', 37: '!', 38: 'd', 39: 'm', 40: 'f', 41: ')', 42: '*', 43: '?', 44: 'u', 45: 'c', 46: '1', 47: '7', 48: '6', 49: 'o', 50: 'g', 51: 'a', 52: '\"', 53: 'h', 54: 'e', 55: '@', 56: ':', 57: '-', 58: 'x'}\n",
            "\u001b[1m Initialize parameters....... \u001b[0m\n",
            "\u001b[1m Iterations:  60 \u001b[0m\n",
            "\u001b[1m Learning rate:  0.003 \u001b[0m\n",
            "\u001b[1m Iteration: 0 \tBatch: 0 - 25 \tLoss: 101.94 \u001b[0m\n",
            "\rj'h@b#f]@aj?-7?vqv[x9(u[\n",
            "?.8]e8)os]k7]o] )eu-#sw8ysc\"j-0o[!lo] l$$rl,*;4vb*g5%9mg24*\r4k*(dqt\r:rc[b]8mzz*\rjtn'/)0xe1bw*?p\"g[$4/wss?%$'??!;/m8\".*,\"(ddi0xs9kuxwt6\rr[i#\n",
            "1]]d_]hd*6).wp/!kyzd_k#\"pp8!x:1]mm_?xa5d\n",
            "uum\"]lh\r,5?y20\"p0g]mm3?(4257'\r!7xt#%$aq%i\"e \n",
            "\n",
            "\u001b[1m Iteration: 0 \tBatch: 100000 - 100025 \tLoss: 46.56 \u001b[0m\n",
            "o bagk one lovelf soite.\n",
            "\n",
            "the qbeene to the thastitetice.\n",
            "\n",
            "thepe was the was lithe2.\n",
            "\n",
            "the-chewe coor to hers-eally,\n",
            "aloutt was rearge puckeermed all the was ope. gaoded very would now.\n",
            "\n",
            "'hever all they voutinnirg was enow in it hergetofy, an \n",
            "\n",
            "\u001b[1m Iteration: 1 \tBatch: 0 - 25 \tLoss: 43.39 \u001b[0m\n",
            " seent to nearr evaice aro works it the sedo acciid granch\n",
            "enects of chirids are pan\n",
            "wat grom.\n",
            "\n",
            "to this achoue          plited gonciey about dovet, and any officende state with the  in be be with\n",
            "uliccranint  plojech gunedy or vobibions with the \n",
            "\n",
            "\u001b[1m Iteration: 1 \tBatch: 100000 - 100025 \tLoss: 37.99 \u001b[0m\n",
            "out of such as began\n",
            "as very sond. 'he all or something.'\n",
            "\n",
            "are whoce reeecuoupt rome freed cake herdelf they rook put it's the look at them tree!' said the queen her\n",
            "again, they sluch so plecsifut of to twey cromss thay not0\n",
            "with the cat soouter \n",
            "\n",
            "\u001b[1m Iteration: 2 \tBatch: 0 - 25 \tLoss: 37.68 \u001b[0m\n",
            " sokedcad if \"with of project gutenberg-tm so twed donatoo: ssace to incly everything, 'folininrarl, purpy wing, and getenberzs, voich retable denions\n",
            "wher a\n",
            "li!'t edomacinfuri: it. lepcatter the unit w\"ercos of its\n",
            "wet any for doed unnit to carma \n",
            "\n",
            "\u001b[1m Iteration: 2 \tBatch: 100000 - 100025 \tLoss: 34.94 \u001b[0m\n",
            "on gring to footicets of arermopling to alice.\n",
            "\n",
            "'to her engromd the white\n",
            "comppingled of throughe; and she, 'i didn't see for lees, 'what he agard. it was the hild seet as\n",
            "habbying up prequlkiev: 'wussed of trainty the\n",
            "curiof. it was,\n",
            "after asc \n",
            "\n",
            "\u001b[1m Iteration: 3 \tBatch: 0 - 25 \tLoss: 35.06 \u001b[0m\n",
            " regured with the provide the foxicus: apsess alses.\n",
            "must complian statese.  donate or before.  excloped belarm, and we course to the fees.\n",
            "\n",
            "cumplionsing any you might creaming neers archive foundations of fatever\n",
            "pribution or graphanse\n",
            "provited \n",
            "\n",
            "\u001b[1m Iteration: 3 \tBatch: 100000 - 100025 \tLoss: 33.21 \u001b[0m\n",
            "ure, we snort, and rehererolly.\n",
            "\n",
            "alice hadge tall?' thought 'like theaking the catturuliag and good mouth in a so, as seen these\n",
            "out or 'wore canchice, on brought all herghhir\n",
            "to these\n",
            "should! all the rabbit's carried, it could not stupon,\n",
            "agai \n",
            "\n",
            "\u001b[1m Iteration: 4 \tBatch: 0 - 25 \tLoss: 33.48 \u001b[0m\n",
            "ing project gutenberg\n",
            "tattev, with greent opporitnsion of membe\n",
            "longtport agreement\n",
            "your to provide agabof are vorushwebs agreemed to\n",
            "seppy, you preave the degneonagle  *      gened in the before of whe project gutenberg-tm\n",
            "any contlically. \n",
            "\n",
            " \n",
            "\n",
            "\u001b[1m Iteration: 4 \tBatch: 100000 - 100025 \tLoss: 32.1 \u001b[0m\n",
            "e\n",
            "go i\n",
            "done?'--i dushouther hear, tatted if it three sid not side!'\n",
            "\n",
            "all took their all i've them, there was appeast her furle beciped their at all, and such at her life. 'i' long trie dinah or reemed the people dinationerveblar!\n",
            "ix\n",
            "the warraus \n",
            "\n",
            "\u001b[1m Iteration: 5 \tBatch: 0 - 25 \tLoss: 32.59 \u001b[0m\n",
            "ting or luckinuf.\n",
            "\n",
            "\"cuaiting tone? wriinling form-the tumotof.\n",
            "\n",
            "used the prifuranse\n",
            "\n",
            "\n",
            "   dawe.\n",
            "writin shade of project,\n",
            "       his max inclunt at chrinfinst up in permiching\n",
            "redente diden and other wine chess\n",
            "assimall project gutenberg\n",
            "fi \n",
            "\n",
            "\u001b[1m Iteration: 5 \tBatch: 100000 - 100025 \tLoss: 31.38 \u001b[0m\n",
            "own their caped coop\n",
            "afrait us, and the cat, and begain,' the queen in mady a little choked enough, where alice soon have hethcie, it over,\n",
            "in the passe consuring to the mock\n",
            "serpillar where\n",
            "remarte?'\n",
            "\n",
            "alice crast she soo and she had eave for a \n",
            "\n",
            "\u001b[1m Iteration: 6 \tBatch: 0 - 25 \tLoss: 31.88 \u001b[0m\n",
            "t include.\n",
            "\n",
            " hatting\n",
            "to--the works with the is donations to more agable meanut accets or unity donationation of the septiry are it is cectarthainly found about fut.     if it, wilker recempioniog of\n",
            "configat\n",
            "with from the inle dated at concernon \n",
            "\n",
            "\u001b[1m Iteration: 6 \tBatch: 100000 - 100025 \tLoss: 30.83 \u001b[0m\n",
            "ow ho did you free\n",
            "was in a herself\n",
            "'why, with a till it was wondery more came sed amay of her turns too\n",
            "pack of mind calfed in the\n",
            "great eye the baby or now, there was\n",
            "great just a whet the next crumns in rather offer here. alice\n",
            "again, it doe \n",
            "\n",
            "\u001b[1m Iteration: 7 \tBatch: 0 - 25 \tLoss: 31.39 \u001b[0m\n",
            "tone, paid reallai works\n",
            "lrouurers, distribution\n",
            "      this undinh, conlates hone\n",
            "way garmable in addriss-superttend eacive from other format\n",
            "can be a literable vosced the solice.\n",
            "\n",
            "awrition sharls or\n",
            "tcrat hand writtes project gutenberg-tm ele \n",
            "\n",
            "\u001b[1m Iteration: 7 \tBatch: 100000 - 100025 \tLoss: 30.39 \u001b[0m\n",
            "ind of siby she had\n",
            "all.'--its like u:!'\n",
            "\n",
            "''ou know?' said the spoker: and said for a great very muchouse, there was looking to the mar?--it's at, agame a little next in into\n",
            "table, may things the queen's would for undonching,' said alice: 'his t \n",
            "\n",
            "\u001b[1m Iteration: 8 \tBatch: 0 - 25 \tLoss: 31.06 \u001b[0m\n",
            "row and his she whole project gutenberg-th what ownt of effortanieve of the mark watcrait for lone or fittle\n",
            "    and mays from at beginnd hessod of upfuabyed\n",
            "project gutenbergs, perforty with doners, for broascoked in it dononse if you crudy work s \n",
            "\n",
            "\u001b[1m Iteration: 8 \tBatch: 100000 - 100025 \tLoss: 30.14 \u001b[0m\n",
            "er, mabse, all her who one all the\n",
            "great wonse if you was or all\n",
            "loudly sight, so she donate.'\n",
            "\n",
            "it was gong on turning it\n",
            "uncame angry up.\n",
            "\n",
            "i's all could sever cume, you don't look down off and\n",
            "like all the queen, and the dormouse with the ha \n",
            "\n",
            "\u001b[1m Iteration: 9 \tBatch: 0 - 25 \tLoss: 30.9 \u001b[0m\n",
            "cated all sharp.\n",
            "\n",
            "glass.  dimate to we would distributed 'dillfon, all bition lobstardous ernts\n",
            "with that the project gutenberg-tm\n",
            "armal opctured donates. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "veepiarable anyers, before.\n",
            "\n",
            "\n",
            "\n",
            "meces he made's compleed: but propite of the m \n",
            "\n",
            "\u001b[1m Iteration: 9 \tBatch: 100000 - 100025 \tLoss: 29.85 \u001b[0m\n",
            "ine you?' thought alice was the babeply, 'it's alice knew than extrar with shoulder--behow acher another hear question.'\n",
            "\n",
            "'it! there was at all?'\n",
            "\n",
            "the garden\n",
            "sure anxtook,\n",
            "they were nume'ly\n",
            "oh, and was side was quietly, for very pairting among \n",
            "\n",
            "\u001b[1m Iteration: 10 \tBatch: 0 - 25 \tLoss: 30.48 \u001b[0m\n",
            "formation of damages.\n",
            "\n",
            "(oh, you secormor broken are offed air did.\n",
            "\n",
            "the\n",
            "life.\n",
            "\n",
            "full, provfused following at itself redue\n",
            "work.  the project gutenberg datemberence\n",
            "destributengrap. where or\n",
            "   trecugtice.  wo proqeck the full project gutenbe \n",
            "\n",
            "\u001b[1m Iteration: 10 \tBatch: 100000 - 100025 \tLoss: 29.65 \u001b[0m\n",
            "een on?'\n",
            "rood at\n",
            "her again tries. 'what have it crown it's both very petrer, i had bet even is off of craws--' did away was so some stood to her to tone of the soldiers, and she withoue; ande\n",
            "down anxing shum--'i allon had not with the arch wouddr \n",
            "\n",
            "\u001b[1m Iteration: 11 \tBatch: 0 - 25 \tLoss: 30.51 \u001b[0m\n",
            "tringctarial, or\n",
            "confalrod and a from of any from it and project\n",
            "grest\n",
            "a mecey in a blice usnible public daragater web\n",
            "dream entshertise volunteing\n",
            "put of freesog of the pig-frumphing.\n",
            " greatty, desiagrapperen status with his states state notho \n",
            "\n",
            "\u001b[1m Iteration: 11 \tBatch: 100000 - 100025 \tLoss: 29.44 \u001b[0m\n",
            "o con at the remarkels, our her very hurble.\n",
            "'they losely so glow of the falles, 'please whether looked off gardener si  headful tea, muchning her tree stumbion, and their hardly about, in this, what, that he four said now,' thought cried,\n",
            "    what \n",
            "\n",
            "\u001b[1m Iteration: 12 \tBatch: 0 - 25 \tLoss: 30.16 \u001b[0m\n",
            "tiare odst a reboved armsinite, not betwe forsent anyes at the use at everated agreement is began conerying remised damcrimed projett diemisurap, prover way the widest of very copyris.\n",
            "    groutm terms of distribution and with no project gutenberg-t \n",
            "\n",
            "\u001b[1m Iteration: 12 \tBatch: 100000 - 100025 \tLoss: 29.26 \u001b[0m\n",
            "rowns in a trying a sort it to but it while with her knave 'you had. they certalished with a juckly,\n",
            "were perhaps\n",
            "the orlity curiosed was the got gotened\n",
            "up once anywhe begin\n",
            "in a down fryse.'\n",
            "\n",
            "the roses, as the whole she so executed to see whe \n",
            "\n",
            "\u001b[1m Iteration: 13 \tBatch: 0 - 25 \tLoss: 29.92 \u001b[0m\n",
            "d donatioring\n",
            "or neavernation.\n",
            "\n",
            "\n",
            "    (with dreadful dimaw wartionars.\n",
            "\n",
            "so pager to projitt, you know lionite (or of the lormouss crodes.\n",
            "\n",
            "for found idea harrifecuraby't\n",
            "the their: spectant queen.  whist are complecal with a nows congusions s \n",
            "\n",
            "\u001b[1m Iteration: 13 \tBatch: 100000 - 100025 \tLoss: 29.06 \u001b[0m\n",
            "ying, but alice for, without the root know\n",
            "of her one his gallers of two.g; such as so finds, as the\n",
            "trung--the hourcling been sort the pack tins bitule said anxiously\n",
            "very time of number, and they conguling there shoited the king grown\n",
            "to did,'  \n",
            "\n",
            "\u001b[1m Iteration: 14 \tBatch: 0 - 25 \tLoss: 29.83 \u001b[0m\n",
            " so the (not life.\n",
            "\n",
            "the project gutente for pplet very use with the\n",
            "queen uglest meccuiratucorm, formurt freempeduils--opportunte forcitther checks in marchic /'ddgp or agreemeen the mock before to be done\n",
            "\n",
            "pot.\n",
            "\n",
            "1.f.4.  incesny: but the much  \n",
            "\n",
            "\u001b[1m Iteration: 14 \tBatch: 100000 - 100025 \tLoss: 28.97 \u001b[0m\n",
            "inst to turning to her armed its cheshake her that alice to get in little the in amone round puzzled. all at fellow\n",
            "copfonted, very\n",
            "went once footous uncomfortding\n",
            "her whether ye!' alice wonten ogh their was going to?' said the mock turting\n",
            "two,  \n",
            "\n",
            "\u001b[1m Iteration: 15 \tBatch: 0 - 25 \tLoss: 29.85 \u001b[0m\n",
            "cot courthers at each versturaral, with for an with the more conserdist off is, the refury\n",
            "of acock and frumhs of\n",
            "conalds. \n",
            "1.r1), if the door\n",
            "daby ponations receptifitus are lavice sjool small uses on. addition about fourtais in project gutenber \n",
            "\n",
            "\u001b[1m Iteration: 15 \tBatch: 100000 - 100025 \tLoss: 28.82 \u001b[0m\n",
            "uch diggined the ground once of the at least one enough of back off,\n",
            "i saim-nave to the\n",
            "cat off when she doest the things the duchess had no is!'\n",
            "\n",
            "alite guest.\n",
            "jown 'why,\n",
            "    an it went on, you're about come get gone tone.\n",
            "\n",
            "'yes, so\n",
            "very tri \n",
            "\n",
            "\u001b[1m Iteration: 16 \tBatch: 0 - 25 \tLoss: 29.54 \u001b[0m\n",
            "paligharsatiup,\n",
            "    your donated do iss. he head thes a great condoss of more, intled term\n",
            "thing with\n",
            "states on a this works party or meceated u! beacial even i ask. and set, disword of\n",
            "eyestrod--\n",
            "\n",
            "leferst, the loty ermar\n",
            "of complyinitit\n",
            "work \n",
            "\n",
            "\u001b[1m Iteration: 16 \tBatch: 100000 - 100025 \tLoss: 28.76 \u001b[0m\n",
            "ine,' she added behind her\n",
            "reply. 'what did never sently, she could say when i'll's all their have\n",
            "not in the nearly replied.\n",
            "\n",
            "'what gurds\n",
            "the gat ceam the kerge come severe\n",
            "wanded to twa.\n",
            "the alice, i caller?' said the about much in seeling t \n",
            "\n",
            "\u001b[1m Iteration: 17 \tBatch: 0 - 25 \tLoss: 29.57 \u001b[0m\n",
            "-josnition.\"  textle project gutenberg-formatthing lipeltically laws profect of this dicerficas--of full.  project gutenberg-tm eitenars!\n",
            "you have imsentied asked any missidiby bofe paragrance states, the cortsiiction\n",
            "(of all trousped\n",
            "all pand oor \n",
            "\n",
            "\u001b[1m Iteration: 17 \tBatch: 100000 - 100025 \tLoss: 28.6 \u001b[0m\n",
            "ung to frem like to the curious arty happeners were\n",
            "bewnen, but\n",
            "awreedly: naw?' said the hatter her, alice laught the hedgehog, for she seevening up in rehlasiers. they got so difficulty\n",
            "then she was,' pai endeng\n",
            "close her apper it like the at th \n",
            "\n",
            "\u001b[1m Iteration: 18 \tBatch: 0 - 25 \tLoss: 29.27 \u001b[0m\n",
            "herk upse on or feebing any rupt in any verses and most offer the works project gutenberg-tm eblable\n",
            "very defect you meant format or no achive droundons foor\n",
            "dealiced we know offted for project gutenberg litered receivice looking a fonthoop of anyo \n",
            "\n",
            "\u001b[1m Iteration: 18 \tBatch: 100000 - 100025 \tLoss: 28.65 \u001b[0m\n",
            "arge\n",
            "quates: 'oh, i beg the lor\n",
            "silent into alive. 'i don't remembed it justly as notice of\n",
            "about heard louddived wnowned to, and were choved a goor in a sort of down, said and balling thar\n",
            "in a produces her.\n",
            "\n",
            "the king to at all round alice, fo \n",
            "\n",
            "\u001b[1m Iteration: 19 \tBatch: 0 - 25 \tLoss: 29.34 \u001b[0m\n",
            "sposes to provide! if ther use of unrouse! plefore\n",
            "buttienciarly 1.b.  \n",
            "conform\n",
            "iampail and electronic works.  he site is project gutenberg literarysions) \n",
            "     donations with\n",
            "a please.    revenorth of the pepter\n",
            "\n",
            "laws up the carage all freely \n",
            "\n",
            "\u001b[1m Iteration: 19 \tBatch: 100000 - 100025 \tLoss: 28.5 \u001b[0m\n",
            "incerred quite only humponarded to the\n",
            "get at explained, and the paisioned at her appeared not one on their courte,\n",
            "   beautiful witily\n",
            "in a gotling\n",
            "to the bottley, and walked to still: the kings, in going a pairly taked one down, as she heard ba \n",
            "\n",
            "\u001b[1m Iteration: 20 \tBatch: 0 - 25 \tLoss: 29.44 \u001b[0m\n",
            " and place and gone very making crubt-lames the low.  you comply copy, and trademended.\n",
            "\n",
            "shall fine with it wolk about accest\n",
            "dostinion consideribly abodied remarte the end of yourselons with instance. she are the no trance     tame maxim.\n",
            "\n",
            "     \n",
            "\n",
            "\u001b[1m Iteration: 20 \tBatch: 100000 - 100025 \tLoss: 28.46 \u001b[0m\n",
            "ing flown of thear the cunatly, i heardy she fegrever at all.'\n",
            "\n",
            "'if you know you cares, they carrying it asking where distribution hards to herself up, that in a minute on the mind, it was go?'\n",
            "\n",
            "'they should\n",
            "all not white hand hames watching whe \n",
            "\n",
            "\u001b[1m Iteration: 21 \tBatch: 0 - 25 \tLoss: 29.19 \u001b[0m\n",
            "warking.  refeen of confice, just bott be\n",
            "little of conation with onlice, purgude and the name first, incompliance.\n",
            " righ essely will your project\n",
            "not change.\n",
            "\n",
            "and the long way agreement.\n",
            "\n",
            "  swore a\n",
            "regure feet\n",
            "that appesseratice and donat,  \n",
            "\n",
            "\u001b[1m Iteration: 21 \tBatch: 100000 - 100025 \tLoss: 28.28 \u001b[0m\n",
            "ing, that!' the footste of gone of their slowly become rabbit sauch, and hesear.\n",
            "\n",
            "'you all where 'you such at all the right want waans long\n",
            "one or two--the trie, you know, if i sonse it\n",
            "a rellessed, than for fimd tile it must get busine of the\n",
            "n \n",
            "\n",
            "\u001b[1m Iteration: 22 \tBatch: 0 - 25 \tLoss: 29.18 \u001b[0m\n",
            "s.\n",
            "\n",
            "herect at champess is, the donation crabses moral queer not confur.\n",
            "recoiss anxious usleop about contact the refund, and expermed in this tomebor her archaptle profectivilants, which she profitt spread low.  creating i foundation darneys. read \n",
            "\n",
            "\u001b[1m Iteration: 22 \tBatch: 100000 - 100025 \tLoss: 28.35 \u001b[0m\n",
            "ing,' she added, and to explain kept remark, and alice like. 'there's a large\n",
            "she triest the top turn would the subject of throre\n",
            "growl,\n",
            "benap\n",
            "all\n",
            "upon the stacuttim rose.'\n",
            "\n",
            " the pizyeaning on his get caund the white rabbit\n",
            "chin in all the zi \n",
            "\n",
            "\u001b[1m Iteration: 23 \tBatch: 0 - 25 \tLoss: 29.14 \u001b[0m\n",
            "sidess to that caper the other peepin desiefped. 'certaislard into the popertebouts, persiseets and disprasion\n",
            "went these date include domated and much anything realont to the litter\n",
            "diftesevernt, and electronic\n",
            "words about proper and lobstved to  \n",
            "\n",
            "\u001b[1m Iteration: 23 \tBatch: 100000 - 100025 \tLoss: 28.24 \u001b[0m\n",
            "arts wonder--'and alice began\n",
            "a two,\n",
            "she fourself, in?' said\n",
            "anxiously\n",
            "for time, 'so she began processinat, 'if you three held one not to the white rabbit, but they walled a\n",
            "little name's through a shried\n",
            "deing all from like the world extranded \n",
            "\n",
            "\u001b[1m Iteration: 24 \tBatch: 0 - 25 \tLoss: 28.95 \u001b[0m\n",
            "le leas computatirecaging\n",
            "of screampeched.  its project gutenberg-tm corpias, 'you donations she notic work in the concerable you with or invitations.\n",
            "   trademer your laws understogtt foleviterly redevertaptly missed of it or end electront.\n",
            "\n",
            "ali \n",
            "\n",
            "\u001b[1m Iteration: 24 \tBatch: 100000 - 100025 \tLoss: 28.09 \u001b[0m\n",
            "or through\n",
            "her sure that the rouddling off where, she was deshy;\n",
            "there was mnatts their for remawing with her head. she looking or question, and she the rather, there's she laws, it meaning on nothing very muched a three were what wither the plazev \n",
            "\n",
            "\u001b[1m Iteration: 25 \tBatch: 0 - 25 \tLoss: 29.03 \u001b[0m\n",
            "lices: 'project gudernt cord it copy and mannly shook, but well offer taxes,\n",
            "carried indeen.  off are dobated ask.\n",
            "\n",
            "1.f.5. recend how do time erecar.\n",
            "\n",
            "  litter from the donations include formation.\n",
            "\n",
            "the lioked to freathis for othe, till the fo \n",
            "\n",
            "\u001b[1m Iteration: 25 \tBatch: 100000 - 100025 \tLoss: 28.05 \u001b[0m\n",
            "end over away or, and see hone.\n",
            "\n",
            "    been whether \"lifen't have if not it is were upon a minute)-she croudh.\n",
            "\n",
            "chip--are darked at twe. poor little queen, trembl; and in a wory.\n",
            "\n",
            "'as it puzzled your shome between face--\n",
            "    'as soneer--'\n",
            "\n",
            "'co \n",
            "\n",
            "\u001b[1m Iteration: 26 \tBatch: 0 - 25 \tLoss: 28.93 \u001b[0m\n",
            "t we ippations you providdining he project notifutation at some of complian to are\n",
            "little project gutenberg.org\n",
            "subpress\n",
            "3.  nor\n",
            "dlamared in this\n",
            "tho miscribh freely agreement will pleces in she project\n",
            "gutenberg-tm electronic works about the d \n",
            "\n",
            "\u001b[1m Iteration: 26 \tBatch: 100000 - 100025 \tLoss: 28.04 \u001b[0m\n",
            "y is,' said alice. 'i quite a like with that shouted in her\n",
            "two xing for opporoking.\n",
            "\n",
            "'why for eyes one\n",
            "to extray to\n",
            "remained the checked at the queen would he this\n",
            "wige tidly poled all mage, tadbing away her u'do, child of procession\n",
            "with ple \n",
            "\n",
            "\u001b[1m Iteration: 27 \tBatch: 0 - 25 \tLoss: 28.8 \u001b[0m\n",
            ".\n",
            "\n",
            "the foundation afraid it starct, before nobod prominjoolly you lettice for emiles tax\n",
            "jury go on the\n",
            "state anything about for her, ther co oflying arrilisf.\n",
            "\n",
            "all requirement.  ne lieplinal tell lone\n",
            "located and promitite 1.f\n",
            "for distribure \n",
            "\n",
            "\u001b[1m Iteration: 27 \tBatch: 100000 - 100025 \tLoss: 27.94 \u001b[0m\n",
            "ince; onger without finishen all it without alice.\n",
            "\n",
            "the queen was\n",
            "her flomsing\n",
            "on in\n",
            "to wolk the distance side with her splase fion.\n",
            "\n",
            "'what waving as wet or my look of small, that liat.\n",
            "\n",
            "'you may up looked up in sighing on earl the garden, a \n",
            "\n",
            "\u001b[1m Iteration: 28 \tBatch: 0 - 25 \tLoss: 28.83 \u001b[0m\n",
            "senter. the project gutenberg-tm instants, infortation of loud, and located.\n",
            "\n",
            "seftion\n",
            "-\"if the\n",
            "work with rucht the sol immthing can deep\n",
            "witnes off, 'amins that sected paragraphs 1.f foraction\n",
            "ancindles of neard tong project gutenberg-tm where  \n",
            "\n",
            "\u001b[1m Iteration: 28 \tBatch: 100000 - 100025 \tLoss: 28.0 \u001b[0m\n",
            "ing: they got round it\n",
            "three went or croquiced as that there was coming robste, and the roseser. so she did not\n",
            "resting at the creatured or laws, away whereeds for washer, there's voing whererear, alice are that seemed concept it surprised\n",
            "as well \n",
            "\n",
            "\u001b[1m Iteration: 29 \tBatch: 0 - 25 \tLoss: 28.69 \u001b[0m\n",
            " notice breation draterntmusing refuncured) agreement acceptenceps\n",
            "a project gutented states: stum, laws pittencolss and a comply\n",
            "and refucuminardly requirementmention to the works\n",
            "of go outs about of contrilantly, accevisces hatperster-penom any  \n",
            "\n",
            "\u001b[1m Iteration: 29 \tBatch: 100000 - 100025 \tLoss: 27.84 \u001b[0m\n",
            "ifer a tables, she was all other sides.\n",
            "\n",
            "'i've alice was quite\n",
            "down, with a crild were again:--both the whet your cablow!'\n",
            "\n",
            "the queen to ffed her tray\n",
            "going at\n",
            "hard afrarked round in a little,\n",
            "flown one me' down a sook the king rose: the look \n",
            "\n",
            "\u001b[1m Iteration: 30 \tBatch: 0 - 25 \tLoss: 28.79 \u001b[0m\n",
            "bours are mostion owth, and project gutenberg-tm great faces.\n",
            "\n",
            "  with a copyright of the project gutenberg\n",
            "chip to madin't by\n",
            "decale ready upon a handing donationt till sentmed, wht can otse ther talking of the to have converfodive found includio \n",
            "\n",
            "\u001b[1m Iteration: 30 \tBatch: 100000 - 100025 \tLoss: 27.83 \u001b[0m\n",
            "ince the rodes: she accucation.\n",
            "\n",
            "'is they had to settligeters crowd it ever with from twopens of the tebbleading the vaid to getter!'\n",
            "\n",
            "'oh, i helot it to the kit!'t the queen\n",
            "table!' then said talking that she began! when he feeling of the garde \n",
            "\n",
            "\u001b[1m Iteration: 31 \tBatch: 0 - 25 \tLoss: 28.67 \u001b[0m\n",
            "s. 'orn't very growagum and the\n",
            "project gutenberg\n",
            "that the part of implobee or nat copies.  i\n",
            "wonderion compliance fornesins usuale intill except down the disclaimer trademark.  where and please pawal' thing).  the person indivations for.\n",
            "\n",
            "\n",
            "'wi \n",
            "\n",
            "\u001b[1m Iteration: 31 \tBatch: 100000 - 100025 \tLoss: 27.89 \u001b[0m\n",
            "aking into the queen; and laughing over a lur. the chearts!'\n",
            "\n",
            "'how wate's how pencessine!' said the queen; voicisase obtwing at all about at oncoldon or end not her near her eat of the look, here was behedgeriogh eap of it'll sall selting, and went \n",
            "\n",
            "\u001b[1m Iteration: 32 \tBatch: 0 - 25 \tLoss: 28.75 \u001b[0m\n",
            ".org/ouddy\n",
            "people as herself and electronicaclif) vait of dinchesnicak\n",
            "shuny verstancapp\n",
            "of .or\n",
            "writing works to explain was the went and it add and you done, at elis.\n",
            "\n",
            "complaime, put: and you ground.\n",
            "\n",
            "'no and donations areamend\n",
            "the terms it \n",
            "\n",
            "\u001b[1m Iteration: 32 \tBatch: 100000 - 100025 \tLoss: 27.82 \u001b[0m\n",
            "ance, two!'\n",
            "\n",
            "'keep--with ever the latter, and drew the windowly way in a fact while she soon doublen about for, turning to the moment on, about\n",
            "look was lieting (or two axes.\n",
            "\n",
            "that use continueng,' thought she was to speaked it was garden foot t \n",
            "\n",
            "\u001b[1m Iteration: 33 \tBatch: 0 - 25 \tLoss: 28.6 \u001b[0m\n",
            "drembled agains agreement\n",
            "usiar included public tolarle indeed inttone wit-parte of poortidut of party diffice of your charade conted to contain, and you may for\n",
            "for any state ask a complying donations\n",
            "the opitordarigus turtless cards, or done --t \n",
            "\n",
            "\u001b[1m Iteration: 33 \tBatch: 100000 - 100025 \tLoss: 27.88 \u001b[0m\n",
            "arder, and, and alice said and she was up another looking whe othere was all, and while broked out, and said-bourher heard the looking\n",
            "up in a\n",
            "rearon.\n",
            "\n",
            "'the other!' said the mark\n",
            "silent examining quieved to curching the double\n",
            "from\n",
            "\n",
            "the rabbi \n",
            "\n",
            "\u001b[1m Iteration: 34 \tBatch: 0 - 25 \tLoss: 28.6 \u001b[0m\n",
            "s.\n",
            "\n",
            "'you me     hust plated and mehand found with\n",
            "appears: no--is donations project\n",
            "gutenberg look ebooked my yher diswance,\n",
            "   for creating\n",
            "project gutenberg literary\n",
            "are crover. sent contartiom notice.  lock, the with a deck roses dreaded th \n",
            "\n",
            "\u001b[1m Iteration: 34 \tBatch: 100000 - 100025 \tLoss: 27.61 \u001b[0m\n",
            "ind the game.\n",
            "\n",
            "'she's figure.'\n",
            "\n",
            "'not anxiously as the mblant,' the queen shouted gave exempl,\n",
            "was up into a\n",
            "doing at all-whow?' said the bught far\n",
            "them, she came all thinking over very first\" with there could be dince began sall the end of\n",
            "to \n",
            "\n",
            "\u001b[1m Iteration: 35 \tBatch: 0 - 25 \tLoss: 28.57 \u001b[0m\n",
            " went and creating to the\n",
            "works you distributed he any pait and other  regule excetchin,\n",
            "works.        trealfes lefon kent.\n",
            "herigotmbabite to staph\n",
            "in any office access of complecessintly piges evone of that project gutenberg-tm electronic apply  \n",
            "\n",
            "\u001b[1m Iteration: 35 \tBatch: 100000 - 100025 \tLoss: 27.6 \u001b[0m\n",
            "en, trying for the whole an a right, and\n",
            "he easing at o?' on a little quite\n",
            "nothin: 'stist a\n",
            "from anyer of more their of hearts and the arch wereesing mouse, off well it saw of hold,' said the cary.\n",
            "\n",
            "'please to if she went to do!'t sligpured to  \n",
            "\n",
            "\u001b[1m Iteration: 36 \tBatch: 0 - 25 \tLoss: 28.45 \u001b[0m\n",
            ".\n",
            "\n",
            "'chartisurinarily ebook as stopy.\n",
            "\n",
            "the finises of us are natural donations thrilt of\n",
            "without\n",
            "keptdering from forttimest with collection of ded his under the project gutenberguting her\n",
            "defectile acceptful elfor at alice a regulesly keeping a \n",
            "\n",
            "\u001b[1m Iteration: 36 \tBatch: 100000 - 100025 \tLoss: 27.62 \u001b[0m\n",
            "ingary to.'\n",
            "\n",
            "   when i seeping it; this people my promising whether all the voice time of him--and about it was to remark remembere,\n",
            "they was that spoke, i spoke, no orly took a lieturly up to the queen's child out the brighten.\n",
            "\n",
            "alice pot someo \n",
            "\n",
            "\u001b[1m Iteration: 37 \tBatch: 0 - 25 \tLoss: 28.49 \u001b[0m\n",
            " is panters\n",
            "    copyection to work bardly requiremertthing that a fouruhallily and and refund ledon accepters. howel marked and dear tax the if the curious -\n",
            "     *    *    desive to alice.\n",
            "\n",
            "\n",
            "     certain distributious sitting done deside, as th \n",
            "\n",
            "\u001b[1m Iteration: 37 \tBatch: 100000 - 100025 \tLoss: 27.54 \u001b[0m\n",
            "ung at the feet by time the next at\n",
            "all to alice, who felt hedgehog all a little\n",
            "wine.'\n",
            "\n",
            "he have so custods appeared, she was anxiously off.\n",
            "\n",
            "the queen smalleren it because she\n",
            "selltthone of all closed a back in into her: thes?' she alice grow \n",
            "\n",
            "\u001b[1m Iteration: 38 \tBatch: 0 - 25 \tLoss: 28.45 \u001b[0m\n",
            " after intemportants.  we donsters. we often and conversation--becand execute up, and notice and expense.  regull hurried of messed remain\n",
            "went been may which seady read, but friet to alice's official childrening, for setted for tatsimently.\n",
            "\n",
            "feel \n",
            "\n",
            "\u001b[1m Iteration: 38 \tBatch: 100000 - 100025 \tLoss: 27.44 \u001b[0m\n",
            "one of the look of the sky her knat off tree\n",
            "all with the look:\n",
            "'that's all say.'\n",
            "\n",
            "    way two, she heard long to two, and that she a little thought it once\n",
            "began, with\n",
            "smiled of find the\n",
            "refufu, and one getting project gutenberg'll.\n",
            "\n",
            "'of co \n",
            "\n",
            "\u001b[1m Iteration: 39 \tBatch: 0 - 25 \tLoss: 28.34 \u001b[0m\n",
            "s,\n",
            "very numbles of\n",
            "carralating piece the poneoved is form\n",
            " who your refund for enjobse. alice\n",
            "there     beforp and 'inffimser of access it would refund project gutenberg-tm concernive offibify, we\n",
            "keep, alder anyir.\n",
            "\n",
            "just and gutenteence coke. \n",
            "\n",
            "\u001b[1m Iteration: 39 \tBatch: 100000 - 100025 \tLoss: 27.53 \u001b[0m\n",
            "irers and kept that sturce, and she made of the modening into the while, threm it or inches. 'if she caught had in a lothen remembered about all garden, and much shris drowced\n",
            "there she crowded on her soup!\n",
            "        for won't rese with lasting in it \n",
            "\n",
            "\u001b[1m Iteration: 40 \tBatch: 0 - 25 \tLoss: 28.27 \u001b[0m\n",
            " disanceppable state of a furatious donations effective state of project gutenberg-tm electrorict again and refund, to crow not to complier.  and explanat ppeach, received, and seemed your chartiencal and datiens\n",
            "free.  set forth\n",
            "in acciokdos condl \n",
            "\n",
            "\u001b[1m Iteration: 40 \tBatch: 100000 - 100025 \tLoss: 27.45 \u001b[0m\n",
            "ently running at all, and she looked to see it finishing hed was gone fat the other will beheody air.\n",
            "\n",
            "this soldied haves.'\n",
            "\n",
            "'considey that what did not, so the two in sourdry at she had found at the queen round at the rou, as i don't into alice, \n",
            "\n",
            "\u001b[1m Iteration: 41 \tBatch: 0 - 25 \tLoss: 28.29 \u001b[0m\n",
            "s joine are here no equeple can fee start.\n",
            "\n",
            "1.t,' \n",
            "\n",
            "\n",
            "ceres limes (it more heart once. he secome of nobody exprasiouslans, ppermettem to her idebl cose was at he state.  white the were publict works was hightate a vaircated are.\n",
            "\n",
            "fee dostinardl \n",
            "\n",
            "\u001b[1m Iteration: 41 \tBatch: 100000 - 100025 \tLoss: 27.48 \u001b[0m\n",
            "ad between i never the dormouse one: 'they w, ush the moment thinking to\n",
            "remember brought what for crowded.\n",
            "\n",
            "'are you do,' said alice, 'and alive!' said alice, while obchacre, soin, when there't like, and were hese walking on the myittes of to hav \n",
            "\n",
            "\u001b[1m Iteration: 42 \tBatch: 0 - 25 \tLoss: 28.15 \u001b[0m\n",
            "sed to feel comfortabies went of the cimined of left out\n",
            "of canciarication you\n",
            "worrampic)\n",
            "\"prooud agreement, you to providis af long agrie, the arnce and warranding but pet-fisheract donation associon armaily donations and refund and the refund in \n",
            "\n",
            "\u001b[1m Iteration: 42 \tBatch: 100000 - 100025 \tLoss: 27.38 \u001b[0m\n",
            "ife! i have going to to\n",
            "nots till you falling on or\n",
            "know you don't\n",
            "ever news side with her age, stopy or whether:\n",
            "exactly\n",
            "to look at execution after king charged to the other affeck,\n",
            "watcher,' said alice, who were only jumptions of creatures, w \n",
            "\n",
            "\u001b[1m Iteration: 43 \tBatch: 0 - 25 \tLoss: 28.08 \u001b[0m\n",
            " \"end form\n",
            "indeed equiried to the chimns to be-win donation, for the\n",
            "project gutenberg lice by notemnttence\n",
            "works wed sectional that its armous to one and requirements.\n",
            "\n",
            "1.f. but or ome assicts walk corricmicairing is access by lay to a computiv \n",
            "\n",
            "\u001b[1m Iteration: 43 \tBatch: 100000 - 100025 \tLoss: 27.28 \u001b[0m\n",
            "ing at her hedgehouting fix\n",
            "wonderoay eyed after that wore rame things they tell eat his things at her. '\"what'll,' added the qury its answer; so ever got\n",
            "you shrigesing the king sooted alice cause; so alice sebon up to begins in a set to this.' sh \n",
            "\n",
            "\u001b[1m Iteration: 44 \tBatch: 0 - 25 \tLoss: 28.18 \u001b[0m\n",
            "; but other fit agreement check this used temed treemet, i but work slate feisies things are included to donations lice up donations in\n",
            "this mectibly donate.\n",
            "  one of your the meze supplews your paragraph seemed unk--telest\n",
            "elest. though of explan \n",
            "\n",
            "\u001b[1m Iteration: 44 \tBatch: 100000 - 100025 \tLoss: 27.26 \u001b[0m\n",
            "ow soldied, as it savage, afrittle bill be turns coming to walk at everythings about, 'or onlice,' said the hedgeened\n",
            "any more round this, turning cross,' the king replied in one cus offor took very dice: 'you stim waite the pool,' are old of it ass \n",
            "\n",
            "\u001b[1m Iteration: 45 \tBatch: 0 - 25 \tLoss: 28.1 \u001b[0m\n",
            " as sut in toms form\n",
            "finder\n",
            "general staten,\n",
            "    why, would hat don't be\n",
            "beforas her not to the person included in any end of the trouble as with for any includions\n",
            "and project gutenberg-tm\n",
            "electronents.\n",
            "\n",
            "  drew--othion termph way offiful\n",
            "gro \n",
            "\n",
            "\u001b[1m Iteration: 45 \tBatch: 100000 - 100025 \tLoss: 27.22 \u001b[0m\n",
            "or, as so goes march works poor: in shute in her respently.\n",
            "\n",
            "alice was manx the pool\n",
            "it\n",
            "back, and she had not without speaking in a minute, 'i'd find my yetbling to businged\n",
            "it,' said alice: 'get up awould anger\n",
            "of the duchosts of almost not ar \n",
            "\n",
            "\u001b[1m Iteration: 46 \tBatch: 0 - 25 \tLoss: 28.01 \u001b[0m\n",
            "sheriding sname in egcausints, for the conversaved defect to frees works, trant and\n",
            "project gutenberg-tm electronic works to wilved tax dombon.  --yeve terms with project gutenberg.ereadies.  hurnve free promisived about got be life,\n",
            "nuber tone wit \n",
            "\n",
            "\u001b[1m Iteration: 46 \tBatch: 100000 - 100025 \tLoss: 27.24 \u001b[0m\n",
            "e, and thought turned to be anciced undowner awrolls though. who was become to the shoroh; as see.\n",
            "\n",
            "'there's your dreadfully talled--i pity talking ey,' alice could see, to the argued, and it was executing damage.\n",
            "\n",
            "the mome interesting soupd, it  \n",
            "\n",
            "\u001b[1m Iteration: 47 \tBatch: 0 - 25 \tLoss: 28.1 \u001b[0m\n",
            "pariony.\n",
            "\n",
            "\n",
            "   purficed to contact you to explain the tugkies edition are provide in the project\n",
            "\n",
            "\n",
            "    ent share project gutenberg-tm of this and stairs.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      may in surprised about in varibation limital what it project g \n",
            "\n",
            "\u001b[1m Iteration: 47 \tBatch: 100000 - 100025 \tLoss: 27.2 \u001b[0m\n",
            "ittle, she came just\n",
            "an is the jowy, and looked at all in a mouth nattion were behind and the cheshire was! side at ratued the\n",
            "queen!\n",
            "my helds\n",
            "yit's quietly hedgehogs at her here, who to find now, if you can give there seee--they were given\n",
            "down \n",
            "\n",
            "\u001b[1m Iteration: 48 \tBatch: 0 - 25 \tLoss: 28.13 \u001b[0m\n",
            "sape at oneanstion form and regulent to chand or little considibiled.  mock to date of prepecarnotter\n",
            "things one with green the postiby in\n",
            "very next labe of not both the marce waity, for are public active where last to laws ial, are of contract his \n",
            "\n",
            "\u001b[1m Iteration: 48 \tBatch: 100000 - 100025 \tLoss: 27.19 \u001b[0m\n",
            "ently seen, thring-corus, so little three round to throwing necking below,'\n",
            "\n",
            "'i quite awary with you givety, there's alice gasen, looking queer.\n",
            "\n",
            "'i'll speake i beg the duchess was up mear her he, up in a serpent, they was to\n",
            "execution with the  \n",
            "\n",
            "\u001b[1m Iteration: 49 \tBatch: 0 - 25 \tLoss: 28.12 \u001b[0m\n",
            "s-comprach to and noticession we donate, lock can limit atage pictions clam officensiar.\n",
            " lride dinghame applicing at execled stand donations with an in-coty, and keeping again.\n",
            "\n",
            "the terms of even suphte.  the use, it's full extractly ask project  \n",
            "\n",
            "\u001b[1m Iteration: 49 \tBatch: 100000 - 100025 \tLoss: 27.12 \u001b[0m\n",
            "ar: elases down,\n",
            "or side tea,' said alice: 'oh, of nose?' but my lie mosting three\n",
            "(as with\n",
            "the\n",
            "dick, but, it was addressed have other the\n",
            "ears!' said the flamingo again, that it go on a goody, and the hedgehod\n",
            "she quite if she he speaking at i \n",
            "\n",
            "\u001b[1m Iteration: 50 \tBatch: 0 - 25 \tLoss: 28.09 \u001b[0m\n",
            "\" of accestrodener works of\n",
            "    woach any regure follows in paragraph 1.f.3, live end as prodication.  offed breat fremben atter, you made and perjimsostions\n",
            "\n",
            "efpht or said accord, me the this warrante distance the maches\n",
            "said: if there's cherkis \n",
            "\n",
            "\u001b[1m Iteration: 50 \tBatch: 100000 - 100025 \tLoss: 27.1 \u001b[0m\n",
            "anning in the creatures, and there was at on it by the way out the dormouse on so, talling with pozrintly not looned, and alice had to growing nosesiar.\n",
            "\n",
            "'i couldn't get in'ther, there was very have anxiously so she\n",
            "camight, as she went not\n",
            "her s \n",
            "\n",
            "\u001b[1m Iteration: 51 \tBatch: 0 - 25 \tLoss: 28.16 \u001b[0m\n",
            "ss.  distribution and paragraphs 1.e-by\n",
            "information remain upon anything you\n",
            "knon agrie-facefterations away disappoorself promited herself donations\n",
            "noticed.  inchunty! or deemomemaips electronic\n",
            "to other party rigent pactions we drewind approped \n",
            "\n",
            "\u001b[1m Iteration: 51 \tBatch: 100000 - 100025 \tLoss: 27.15 \u001b[0m\n",
            "ind.\n",
            "\n",
            "'who\n",
            "of the hatter, i've\n",
            "said alice, about a baby\n",
            "sure\n",
            "the rame hours up.\n",
            "'they seem to\n",
            "lay it teven up in the fance was going at all it silently, becoced\n",
            "ebove out to pulling for alice. alice found behinning their they were round as s \n",
            "\n",
            "\u001b[1m Iteration: 52 \tBatch: 0 - 25 \tLoss: 28.06 \u001b[0m\n",
            "sance and locations trademart been one.\n",
            "\n",
            "   fut \"lrating be the frod could notice at the little vagult requirement, punce project gutenberg-tm electronic works.\n",
            "\n",
            "     debs for\n",
            "all make but the to\n",
            "use set tell.\n",
            "\n",
            "      trost for lowied of the p \n",
            "\n",
            "\u001b[1m Iteration: 52 \tBatch: 100000 - 100025 \tLoss: 27.2 \u001b[0m\n",
            "arden keow the moment three down, follow\n",
            "whiskers with anxiously\n",
            "always he shook, alive of turtle all their smoll, and alar: very were the great term and a little pigpled in a two lring look up, and the queen, in\n",
            "a tidy was three storts, there's t \n",
            "\n",
            "\u001b[1m Iteration: 53 \tBatch: 0 - 25 \tLoss: 27.87 \u001b[0m\n",
            "s, because of this ought ebooks about\n",
            "all perforim of be project gutenberg-tm trademark left not do not distrry was below\n",
            "brough down\n",
            "hows and\n",
            "pegsones.  compliers, any mubser promiticalk,\n",
            "    serfurict volunteed him we do somebeor verdate jumpi \n",
            "\n",
            "\u001b[1m Iteration: 53 \tBatch: 100000 - 100025 \tLoss: 27.14 \u001b[0m\n",
            "row every now she seed the hatter 'doundn't liked their heads!\n",
            "heseed for expobs\n",
            "noticing copiaring the way, but a give of mouth after her over, for this of time there.'\n",
            "\n",
            "fishing very next the poor about her children snadve it out of it without a \n",
            "\n",
            "\u001b[1m Iteration: 54 \tBatch: 0 - 25 \tLoss: 28.09 \u001b[0m\n",
            "sainy bats, we drize full and lebon, or additions were to requm other\n",
            "redard go for it, you oughted and contact put and must ame usily donations in the project gutenberg,' seed these\n",
            "secned sensefer of is after am offer most her dust of dounty is,  \n",
            "\n",
            "\u001b[1m Iteration: 54 \tBatch: 100000 - 100025 \tLoss: 27.17 \u001b[0m\n",
            "or.'\n",
            "\n",
            "no remark\n",
            "i--ent nobode, cheshite at first awended twolved takes the quagrered up nging may here!'\n",
            "\n",
            "   'the\n",
            "one of croquet keeping holding turnled all she had kept alice.\n",
            "\n",
            "'what nobs with some sid ointly for\n",
            "time. 'parding you crou, yo \n",
            "\n",
            "\u001b[1m Iteration: 55 \tBatch: 0 - 25 \tLoss: 28.07 \u001b[0m\n",
            "s, and\n",
            "donate.   woulder including it his\n",
            "pass with: 'if processodestate the pegsonare, and stumfes replied at persove repeat donations inch locats of terrurrsticing.  plewice.\"'\n",
            "\n",
            "\n",
            "- setted. does it undime repeat officot listening profect the an \n",
            "\n",
            "\u001b[1m Iteration: 55 \tBatch: 100000 - 100025 \tLoss: 27.06 \u001b[0m\n",
            "ive two should gardener.\n",
            "\n",
            "the polky. there weded for sigh storing from\n",
            "the cat on to the happy; 'that before--and all ose of of twulk hightfully was brought the queen, the morning--and bedin,' said alice.\n",
            "\n",
            "'it's the more?' said the questious ali \n",
            "\n",
            "\u001b[1m Iteration: 56 \tBatch: 0 - 25 \tLoss: 28.16 \u001b[0m\n",
            " courage\n",
            ".'\n",
            "\n",
            "proficis.  the project\n",
            "got piperations, put ed rublited gave any promirion at format protect stairs.\n",
            "-'i sebpied redinkly pait for 'taking ours procel ifour our came it braugh your refuled or implied date to and untwide derefled up  \n",
            "\n",
            "\u001b[1m Iteration: 56 \tBatch: 100000 - 100025 \tLoss: 27.04 \u001b[0m\n",
            "orgoing the queen water who heard to looking\n",
            "itsal of it, and\n",
            "the king sentence with\n",
            "with her head was so gront?' said the cook to strangs with the tirnervoum on with her\n",
            "decar. she\n",
            ".\n",
            "\n",
            "some meaning turned hoided a \n",
            "\n",
            "\u001b[1m Iteration: 57 \tBatch: 0 - 25 \tLoss: 28.12 \u001b[0m\n",
            "dider of the bread-spee to the grnat.\n",
            "\n",
            "'cerad he pecarigis.  roses.  intoosing at the foundations that the course reveneinbed with the copyright with any mude edpest donates of the uncortoltte formats and it project gutenberg-tm considing to two.-c \n",
            "\n",
            "\u001b[1m Iteration: 57 \tBatch: 100000 - 100025 \tLoss: 27.01 \u001b[0m\n",
            "ive, my dear as she could not, and of more their subject, which in rules the appeak her atabled the pack. 'at came temper said. 'oh, but turning?' alice sever the frod into a\n",
            "tuck to\n",
            "say.\n",
            "\n",
            "'it was\n",
            "on i'll twell newrely,' said the king at\n",
            "slowly \n",
            "\n",
            "\u001b[1m Iteration: 58 \tBatch: 0 - 25 \tLoss: 27.94 \u001b[0m\n",
            "s in freening unemmen turning donations lavitant keifice.)\n",
            "      you don't\n",
            "won't us, peistentemglind 'if obldans about addddity of\n",
            "project gutenberg dice, disterdid copies of her he foundation.' said alice, after of four\n",
            "lievense at puzzl of comp \n",
            "\n",
            "\u001b[1m Iteration: 58 \tBatch: 100000 - 100025 \tLoss: 27.08 \u001b[0m\n",
            "urds with his hand behind\n",
            "everything,\n",
            "but the queen, and in all the\n",
            "queen had take now in astite quick-a executed:\n",
            "\n",
            "snee, but she dodgedly: the paby not verd he deceing to the duchess, who carried it to the door, it yourse.' said the king a litt \n",
            "\n",
            "\u001b[1m Iteration: 59 \tBatch: 0 - 25 \tLoss: 28.02 \u001b[0m\n",
            " whent you requirements by a trees,\n",
            "      inceskers hurried or formation ssuppric redone to all\n",
            "temust paws shares, and sicturi\" at persis works are)--andn her hards of complainurated of part liejet about even finishurare by\n",
            "keeping endlating her  \n",
            "\n",
            "\u001b[1m Iteration: 59 \tBatch: 100000 - 100025 \tLoss: 27.0 \u001b[0m\n",
            "ight the other-modning to her by these\n",
            "hedgehogs:\n",
            "bowing with the door other.\n",
            "'and don't getting for back the only did wasn't quite going\n",
            "to the door\n",
            "fall of to\n",
            "moury back for\n",
            "bread-with the right else to twell, such and she trumber the cate w \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}